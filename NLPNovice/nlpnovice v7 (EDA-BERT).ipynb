{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bert-for-tf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config IPCompleter.greedy=True\n\nimport string\nimport numpy as np\nimport pandas as pd\nimport re\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam,Nadam,SGD\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nfrom keras.constraints import unit_norm\n\nimport bert\nimport talos as ta\n\nimport nltk\nimport unicodedata\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":285,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nstop_words=set(stopwords.words('english'))","execution_count":286,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n","execution_count":287,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe, test.describe","execution_count":288,"outputs":[{"output_type":"execute_result","execution_count":288,"data":{"text/plain":"(<bound method NDFrame.describe of          id keyword location  \\\n 0         1     NaN      NaN   \n 1         4     NaN      NaN   \n 2         5     NaN      NaN   \n 3         6     NaN      NaN   \n 4         7     NaN      NaN   \n ...     ...     ...      ...   \n 7608  10869     NaN      NaN   \n 7609  10870     NaN      NaN   \n 7610  10871     NaN      NaN   \n 7611  10872     NaN      NaN   \n 7612  10873     NaN      NaN   \n \n                                                    text  target  \n 0     Our Deeds are the Reason of this #earthquake M...       1  \n 1                Forest fire near La Ronge Sask. Canada       1  \n 2     All residents asked to 'shelter in place' are ...       1  \n 3     13,000 people receive #wildfires evacuation or...       1  \n 4     Just got sent this photo from Ruby #Alaska as ...       1  \n ...                                                 ...     ...  \n 7608  Two giant cranes holding a bridge collapse int...       1  \n 7609  @aria_ahrary @TheTawniest The out of control w...       1  \n 7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n 7611  Police investigating after an e-bike collided ...       1  \n 7612  The Latest: More Homes Razed by Northern Calif...       1  \n \n [7613 rows x 5 columns]>,\n <bound method NDFrame.describe of          id keyword location  \\\n 0         0     NaN      NaN   \n 1         2     NaN      NaN   \n 2         3     NaN      NaN   \n 3         9     NaN      NaN   \n 4        11     NaN      NaN   \n ...     ...     ...      ...   \n 3258  10861     NaN      NaN   \n 3259  10865     NaN      NaN   \n 3260  10868     NaN      NaN   \n 3261  10874     NaN      NaN   \n 3262  10875     NaN      NaN   \n \n                                                    text  \n 0                    Just happened a terrible car crash  \n 1     Heard about #earthquake is different cities, s...  \n 2     there is a forest fire at spot pond, geese are...  \n 3              Apocalypse lighting. #Spokane #wildfires  \n 4         Typhoon Soudelor kills 28 in China and Taiwan  \n ...                                                 ...  \n 3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n 3259  Storm in RI worse than last hurricane. My city...  \n 3260  Green Line derailment in Chicago http://t.co/U...  \n 3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n 3262  #CityofCalgary has activated its Municipal Eme...  \n \n [3263 rows x 4 columns]>)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Cleaing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_whitespace(data):\n    return data.strip()","execution_count":289,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(data):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',data)","execution_count":290,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(data):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',data)","execution_count":291,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(data):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', data)","execution_count":292,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_accented_chars(data):\n    return unicodedata.normalize('NFKD', data).encode('ascii', 'ignore').decode('utf-8', 'ignore')","execution_count":293,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuation(data):\n    table=str.maketrans('','',string.punctuation)\n    return data.translate(table)","execution_count":294,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def single_char(data):\n    new_data=''\n    for w in data:\n        print(w)\n        if len(w) > 1:\n            print(w)\n            new_data = new_data +  \" \" + w\n    \n    return new_data","execution_count":295,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_characters(data, remove_digits=False):\n    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n    data = re.sub(pattern, '', data)\n    return data","execution_count":296,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lower_case(data):\n    return data.lower()","execution_count":297,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenise(data):\n    data = word_tokenize(data)\n    return data","execution_count":298,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stop_words(data):\n    filtered_sentence = [w for w in data if not w in stop_words] \n    return filtered_sentence","execution_count":299,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stemming(data):\n    data.apply(lambda x: [stemmer.stem(e) for e in x])\n    return data","execution_count":300,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatise(data):\n    lemmatizer = WordNetLemmatizer()\n    data.apply(lambda x: [lemmatizer.lemmatize(e) for e in x])\n    return data ","execution_count":301,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nlp_clean(data):\n    data = remove_URL(data)\n    data = remove_html(data)\n    data = remove_emoji(data)\n    data = remove_whitespace(data)    \n    data = remove_accented_chars(data) \n    data = remove_special_characters(data)\n    data = remove_punctuation(data)\n    data = convert_lower_case(data)\n    return data\n    \ndef nlp_tokenise(data):\n    stop_words = set(stopwords.words('english'))\n    data = tokenise(data)\n    data = remove_stop_words(data)\n    return data\n     \ndef nlp_normalise(data):\n    stemmer = SnowballStemmer(\"english\")\n    data = stemming(data)\n    data = lemmatise(data)\n    return data","execution_count":302,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run preprossing steps to clean data\ntrain['text']=train['text'].apply(lambda x : nlp_clean(x))\ntest['text']=test['text'].apply(lambda x : nlp_clean(x))","execution_count":303,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run steps to remove stop words\ntrain['text']=train['text'].apply(lambda x : nlp_tokenise(x))\ntest['text']=test['text'].apply(lambda x : nlp_tokenise(x))","execution_count":304,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rejoin Data after tokenisation \ndef combine_text(list_of_text):\n    combined_text = ''\n    for word in list_of_text:\n        combined_text = combined_text + ' ' + word\n    return combined_text","execution_count":305,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))","execution_count":306,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'].head(30)","execution_count":307,"outputs":[{"output_type":"execute_result","execution_count":307,"data":{"text/plain":"0          deeds reason earthquake may allah forgive us\n1                 forest fire near la ronge sask canada\n2      residents asked shelter place notified office...\n3      13000 people receive wildfires evacuation ord...\n4      got sent photo ruby alaska smoke wildfires po...\n5      rockyfire update california hwy 20 closed dir...\n6      flood disaster heavy rain causes flash floodi...\n7                            im top hill see fire woods\n8      theres emergency evacuation happening buildin...\n9                         im afraid tornado coming area\n10                      three people died heat wave far\n11     haha south tampa getting flooded hah wait sec...\n12     raining flooding florida tampabay tampa 18 19...\n13                      flood bago myanmar arrived bago\n14        damage school bus 80 multi car crash breaking\n15                                            whats man\n16                                          love fruits\n17                                        summer lovely\n18                                             car fast\n19                                      goooooooaaaaaal\n20                                           ridiculous\n21                                          london cool\n22                                          love skiing\n23                                        wonderful day\n24                                             looooool\n25                                   wayi cant eat shit\n26                                        nyc last week\n27                                      love girlfriend\n28                                               cooool\n29                                           like pasta\nName: text, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text'].head(30)","execution_count":308,"outputs":[{"output_type":"execute_result","execution_count":308,"data":{"text/plain":"0                           happened terrible car crash\n1      heard earthquake different cities stay safe e...\n2      forest fire spot pond geese fleeing across st...\n3                 apocalypse lighting spokane wildfires\n4                typhoon soudelor kills 28 china taiwan\n5                                 shakingits earthquake\n6      theyd probably still show life arsenal yester...\n7                                                   hey\n8                                              nice hat\n9                                                  fuck\n10                                       dont like cold\n11                                      nooooooooo dont\n12                                            dont tell\n13                                                     \n14                                              awesome\n15     birmingham wholesale market ablaze bbc news f...\n16               sunkxssedharry wear shorts race ablaze\n17     previouslyondoyintv toke makinwauas marriage ...\n18                                           check nsfw\n19     psa iuam splitting personalities techies foll...\n20            beware world ablaze sierra leone amp guap\n21              burning man ablaze turban diva via etsy\n22     diss song people take 1 thing run smh eye ope...\n23     rape victim dies sets ablaze 16yearold girl d...\n24                                       setting ablaze\n25     ctvtoronto bins front field house wer set abl...\n26     nowplaying alfons ablaze 2015 puls radio puls...\n27     burning rahm lets hope city hall builds giant...\n28     philippaeilhart dhublath hurt eyes ablaze ins...\n29     accident cleared paturnpike patp eb pa18 cran...\nName: text, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.to_csv('../../Data/NLPNovice/DSKtrain.csv', index=False)\n# test.to_csv('../../Data/NLPNovice/DSKtest.csv', index=False)","execution_count":309,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load pre cleaned text files"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load pre cleaned text files\n# train = pd.read_csv('../../Data/NLPNovice/DSKtrain.csv')\n# test = pd.read_csv('../../Data/NLPNovice/DSKtest.csv')\n\n# Convert text column from object to string\ntrain['text'] = train['text'].apply(lambda x : str(x))\ntest['text'] = test['text'].apply(lambda x : str(x))\n\n#Shuffle Data \ntrain = train.sample(frac=1).reset_index(drop=True)\ntest = test.sample(frac=1).reset_index(drop=True)","execution_count":310,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Tensor Flow Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":311,"outputs":[{"output_type":"stream","text":"CPU times: user 7.28 s, sys: 649 ms, total: 7.93 s\nWall time: 7.82 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n# And BERT implementation convert_single_example() at \n\ndef bert_encode(texts, tokenizer, max_len):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens = tokens + [0] * pad_len\n\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        \n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":312,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the architecture of the model.\n# maxlenght BERT has a constraint on the maximum length of a sequence after tokenizing.\n# input token ids (tokenizer converts tokens using vocab file)\n# Input masks (1 for useful tokens, 0 for padding)\n# segment ids (for 2 text training: 0 for the first one, 1 for the second one)\n# pooled_output of shape [batch_size, 768] with representations for the entire input sequences\n# sequence_output of shape [batch_size, max_seq_length, 768] with representations for each input token\n\nmaxlength=130 # Length of longest train tweet\n\ninput_word_ids = Input(shape=(maxlength,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = Input(shape=(maxlength,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = Input(shape=(maxlength,), dtype=tf.int32, name=\"segment_ids\")","execution_count":313,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = bert.tokenization.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":314,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(train.text.values, tokenizer, max_len=maxlength)","execution_count":315,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_input = bert_encode(test.text.values, tokenizer, max_len=maxlength)\ntrain_labels = train.target.values","execution_count":316,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Tensor Flow Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n\nclf_output = sequence_output[:, 0, :]\nout = Dense(1, activation='sigmoid')(clf_output)\n\nmodel = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\noptimizer = Adam(learning_rate=2e-5, beta_1=0.9, beta_2=0.999, amsgrad=False)\nmodel.compile(loss='binary_crossentropy',  optimizer=optimizer, metrics=['accuracy'])","execution_count":317,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"model.summary()","execution_count":318,"outputs":[{"output_type":"stream","text":"Model: \"model_24\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 130)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 130)]        0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 130)]        0                                            \n__________________________________________________________________________________________________\nkeras_layer_6 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n__________________________________________________________________________________________________\ntf_op_layer_strided_slice_29 (T [(None, 768)]        0           keras_layer_6[0][1]              \n__________________________________________________________________________________________________\ndense_43 (Dense)                (None, 768)          590592      tf_op_layer_strided_slice_29[0][0\n__________________________________________________________________________________________________\ndropout_16 (Dropout)            (None, 768)          0           dense_43[0][0]                   \n__________________________________________________________________________________________________\ndense_44 (Dense)                (None, 1)            769         dropout_16[0][0]                 \n==================================================================================================\nTotal params: 110,073,602\nTrainable params: 110,073,601\nNon-trainable params: 1\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc', \n                                            patience=2,\n                                            verbose =1,\n                                            restore_best_weights=True)\n\nhistory = model.fit(\n        train_input, train_labels,\n        validation_split=0.2,\n        callbacks=[callback],\n        epochs=6,\n        batch_size=32)\n\nprint('\\nhistory dict:', history.history)\n    ","execution_count":320,"outputs":[{"output_type":"stream","text":"Train on 6090 samples, validate on 1523 samples\nEpoch 1/6\n6090/6090 [==============================] - 100s 16ms/sample - loss: 0.4400 - accuracy: 0.8034 - val_loss: 0.4173 - val_accuracy: 0.8181\nEpoch 2/6\n6090/6090 [==============================] - 97s 16ms/sample - loss: 0.2654 - accuracy: 0.8997 - val_loss: 0.4692 - val_accuracy: 0.8063\nEpoch 3/6\n6090/6090 [==============================] - 97s 16ms/sample - loss: 0.1274 - accuracy: 0.9537 - val_loss: 0.5679 - val_accuracy: 0.8011\nEpoch 4/6\n 480/6090 [=>............................] - ETA: 1:22 - loss: 0.0519 - accuracy: 0.9821","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-320-b502fe8905ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         batch_size=32)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nhistory dict:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = history.history\nhistory_dict.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()   # clear figure\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install talos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = {'lr': (0.5, 5, 10),\n     'first_neuron':[4, 8, 16, 32, 64],\n     'hidden_layers':[0, 1, 2],\n     'batch_size': (2, 30, 10),\n     'epochs': [150],\n     'dropout': (0, 0.5, 5),\n     'weight_regulizer':[None],\n     'emb_output_dims': [None],\n     'shape':['brick','long_funnel'],\n     'optimizer': [Adam, Nadam, RMSprop]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = ta.Scan(x=train_input,\n            y=train_labels,\n            model=model,\n            grid_downsample=0.01, \n            params=p,\n            dataset_name='NPL',\n            experiment_no='1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict=model.predict(test_input)\nprint(predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict=np.round(predict).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':test['id'].values.tolist(),'target':predict})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('DSKsubmission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!kaggle competitions submit -c nlp-getting-started -f DSKsubmission.csv -m \"DSK NLP with Keras\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}